{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5f4e4c1b",
   "metadata": {},
   "source": [
    "## Desaf√≠o N.3 - Proceso Batch a la Nube (Azure)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44ec6715",
   "metadata": {},
   "source": [
    "### Objetivo\n",
    "\n",
    "Crear un proceso batch que migre los archivos **.parquet** generados de manera local a un contenerdos en la nube (\"Azure Blob Storage\")."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c630ffb",
   "metadata": {},
   "source": [
    "### Tecnologias empleadas\n",
    "\n",
    "| Librer√≠a / M√≥dulo                    | Prop√≥sito                                           |  Descripci√≥n                                                                                                                                                                           |\n",
    "| ------------------------------------ | --------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |\n",
    "| **azure.storage.blob**               | Conexi√≥n a Azure Blob Storage                       | Permite interactuar con contenedores y blobs en Azure. Se utiliza `BlobServiceClient` para crear contenedores y subir los archivos `.parquet` generados al almacenamiento en la nube. |\n",
    "| **pathlib.Path**                     | Manejar rutas de archivos                           | Simplifica la gesti√≥n de rutas locales (`data/`) de forma compatible entre sistemas operativos. Se usa para iterar sobre los archivos `.parquet` en la carpeta de datos.              |\n",
    "| **os**                               | Interactuar con el sistema operativo                | Se emplea para acceder a las variables de entorno (`AZURE_STORAGE_CONNECTION_STRING`, `AZURE_CONTAINER`, `SUPABASE_URL`, `SUPABASE_KEY`, etc.) almacenadas de forma segura.           |\n",
    "| **dotenv (load_dotenv)**             | Cargar variables de entorno desde un archivo `.env` | Permite cargar autom√°ticamente credenciales y configuraciones sensibles sin incluirlas directamente en el c√≥digo fuente.                                                              |\n",
    "| **pandas**                           | Manipulaci√≥n y an√°lisis de datos                    | Se usa para leer los archivos `.parquet` (`employees.parquet`, `jobs.parquet`), limpiar y transformar los datos antes de su carga a la base de datos.                                 |\n",
    "| **psycopg2**                         | Conexi√≥n a bases de datos PostgreSQL                | Biblioteca nativa para interactuar con bases de datos PostgreSQL. En este proyecto, permite conectarse a **Supabase**, que usa PostgreSQL como motor principal.                       |\n",
    "| **psycopg2.extras (execute_values)** | Inserci√≥n eficiente de datos                        | Optimiza la carga masiva de registros desde DataFrames hacia tablas PostgreSQL (Supabase) en una sola instrucci√≥n SQL, reduciendo el tiempo de inserci√≥n.                             |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef034eb6",
   "metadata": {},
   "source": [
    "### Ejecuci√≥n\n",
    "\n",
    "El desarrollo del batch lleva el siguiente proceso: Local /data/*.parquet   -> Azure Blob Storage (Raw Layer / Landing Zone) & Supabase(postgreSQL) -> Desaf√≠o n.4. Para su ejecuci√≥n se desarrollaron los pasos de:\n",
    "\n",
    "1. **Varaibles de entorno**\n",
    "    \n",
    "    Con **dotev** se mantienen las credenciales privadas y seguras evitando su vulnerabilidad en el codigo.\n",
    "    ```python\n",
    "    from azure.storage.blob import BlobServiceClient\n",
    "    from pathlib import Path\n",
    "    import os\n",
    "    from dotenv import load_dotenv\n",
    "    import psycopg2\n",
    "    from psycopg2.extras import execute_values\n",
    "    import pandas as pd\n",
    "\n",
    "\n",
    "\n",
    "    # Cargar variables desde archivo .env\n",
    "    load_dotenv()\n",
    "\n",
    "    # Recuperar valores\n",
    "    CONN_STR = os.getenv(\"AZURE_STORAGE_CONNECTION_STRING\")\n",
    "    CONTAINER = os.getenv(\"AZURE_CONTAINER\")\n",
    "    DATA_DIR = Path(\"data\")\n",
    "    ```\n",
    "    **Nota**: El archivo .env tiene la forma:\n",
    "\n",
    "    AZURE_STORAGE_CONNECTION_STRING=DefaultEndpointsProtocol=...\n",
    "\n",
    "    AZURE_CONTAINER=synthetic-data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86c96dfa",
   "metadata": {},
   "source": [
    "2. **Conexi√≥n Azure Blob Storage**\n",
    "    \n",
    "    Se crea la conexi√≥n con Azure Blob Storage\n",
    "    ```python\n",
    "    from azure.storage.blob import BlobServiceClient\n",
    "\n",
    "    # Crear cliente de servicio\n",
    "    blob_service_client = BlobServiceClient.from_connection_string(CONN_STR)\n",
    "    ```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2111061",
   "metadata": {},
   "source": [
    "3. **Creaci√≥n de contenedores**\n",
    "    \n",
    "    Se crea para la cargar los archivos. Si ya existe o se genera error devuelve mensaje.  \n",
    "    ```python\n",
    "    try:\n",
    "        blob_service_client.create_container(CONTAINER)\n",
    "        print(f\"‚úÖ Contenedor '{CONTAINER}' creado\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Contenedor '{CONTAINER}' ya existe o error: {e}\")\n",
    "    ```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c1546b1",
   "metadata": {},
   "source": [
    "3. **Carga de archivos**\n",
    "    \n",
    "    Se recorre la carpeta **data** para cargar o sobrescribir todos los archivos **.parquet**.  \n",
    "```python\n",
    "  for file_path in DATA_DIR.glob(\"*.parquet\"):\n",
    "    blob_client = blob_service_client.get_blob_client(container=CONTAINER, blob=file_path.name)\n",
    "    with open(file_path, \"rb\") as data:\n",
    "        blob_client.upload_blob(data, overwrite=True)\n",
    "    print(f\"‚úÖ Subido {file_path.name} a Azure Blob Storage\")\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e933be6c",
   "metadata": {},
   "source": [
    "4. **Conexi√≥n segura a Supabase**\n",
    "    \n",
    "    Se establecen las credenciales de conexi√≥n usando variables de entorno.  \n",
    "```python\n",
    "    SUPABASE_HOST = os.getenv(\"SUPABASE_HOST\")\n",
    "    SUPABASE_DB = os.getenv(\"SUPABASE_DB\")\n",
    "    SUPABASE_USER = os.getenv(\"SUPABASE_USER\")\n",
    "    SUPABASE_PASSWORD = os.getenv(\"SUPABASE_PASSWORD\")\n",
    "    SUPABASE_PORT = os.getenv(\"SUPABASE_PORT\", 5432)\n",
    "\n",
    "    conn = psycopg2.connect(\n",
    "        host=SUPABASE_HOST,\n",
    "        dbname=SUPABASE_DB,\n",
    "        user=SUPABASE_USER,\n",
    "        password=SUPABASE_PASSWORD,\n",
    "        port=SUPABASE_PORT\n",
    "    )\n",
    "    cur = conn.cursor()\n",
    "    print(\"‚úÖ Conectado a Supabase\")\n",
    "\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "130b62a6",
   "metadata": {},
   "source": [
    "5. **Carga de tabla**\n",
    "    \n",
    "    Se crea con estructura definida, y se insertan los datos desde los archivos correspondientes, en este caso **deparments.parquet**.  \n",
    "```python\n",
    "   df_departamentos = pd.read_parquet(\"data/departments.parquet\")\n",
    "\n",
    "    cur.execute(\"\"\"\n",
    "    DROP TABLE IF EXISTS departamentos CASCADE;\n",
    "    CREATE TABLE departamentos (\n",
    "        department_id SERIAL PRIMARY KEY,\n",
    "        department_name TEXT,\n",
    "        location TEXT\n",
    "    );\n",
    "    \"\"\")\n",
    "\n",
    "    execute_values(\n",
    "        cur,\n",
    "        \"INSERT INTO departamentos (department_id, department_name, location) VALUES %s\",\n",
    "        df_departamentos.values.tolist()\n",
    "    )\n",
    "    print(f\"‚úÖ Tabla 'departamentos' cargada ({len(df_departamentos)} filas)\")\n",
    "\n",
    "\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "499306df",
   "metadata": {},
   "source": [
    "6. **Creaci√≥n de  llaves for√°neas y cierre de conexi√≥n**\n",
    "    \n",
    "    Se crea llaves foraneas para asegurar la integridad referencial mediante claves for√°neas:.  \n",
    "```python\n",
    "    cur.execute(\"\"\"\n",
    "    DROP TABLE IF EXISTS empleados CASCADE;\n",
    "    CREATE TABLE empleados (\n",
    "        employee_id SERIAL PRIMARY KEY,\n",
    "        first_name TEXT,\n",
    "        last_name TEXT,\n",
    "        email TEXT,\n",
    "        phone_number TEXT,\n",
    "        department_id INT REFERENCES departamentos(department_id),\n",
    "        job_id INT REFERENCES puestos(job_id),\n",
    "        salary NUMERIC,\n",
    "        hire_date DATE\n",
    "    );\n",
    "    \"\"\")\n",
    "\n",
    "    conn.commit()\n",
    "    cur.close()\n",
    "    conn.close()\n",
    "    print(\"üîí Carga finalizada correctamente con relaciones entre tablas\")\n",
    "\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fa078c2",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
